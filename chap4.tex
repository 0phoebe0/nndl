% file: chap4.tex

\chapter{神经网络可以计算任何函数的可视化证明}
\label{ch:VisualProof}

神经网络的一个最显著的事实就是它可以计算任何的函数。也就是说，假设某个人给你某种
复杂而奇特的函数，$f(x)$：
\begin{center}
  \includegraphics{function}
\end{center}

\label{basic_network_precursor}不管这个函数是什么样的，总会确保有一个神经网络能够
对任何可能的输入 $x$，其值 $f(x)$ （或者某个足够准确的近似）是网络的输出，例如：
\begin{center}
  \includegraphics{basic_network}
\end{center}

即使函数有很多输入和输出，$f = f(x_1, \ldots, x_m)$，这个结果都是成立的。例如，这
个网络计算一个具有 $m = 3$ 个输入和 $n = 2$ 个输出的函数：
\begin{center}
  \includegraphics{vector_valued_network}
\end{center}

结果表明神经网络拥有一种\emph{普遍性}。不论我们想要计算什么样的函数，我们都确信存
在一个神经网络可以计算它。

而且，这个普遍性定理甚至在我们限制了神经网络只在输入层和输出层存在一个中间层的情
况下成立。所以即使是很简单的网络架构都极其强大。

普遍性定理在使用神经网络的人群中是众所周知的。但是它为何正确却不被广泛地理解。现
有的大多数的解释都具有很强的技术性。例如，有一篇原始的论
文\footnote{\href{http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf}{Approximation
    by superpositions of a sigmoidal function}, by George Cybenko (1989). The
  result was very much in the air at the time, and several groups proved closely
  related results. Cybenko's paper contains a useful discussion of much of that
  work. Another important early paper is
  \href{http://www.sciencedirect.com/science/article/pii/0893608089900208}{Multilayer
    feedforward networks are universal approximators}, by Kurt Hornik, Maxwell
  Stinchcombe, and Halbert White (1989). This paper uses the Stone-Weierstrass
  theorem to arrive at similar results.}使用了%
\href{https://zh.wikipedia.org/wiki/哈恩－巴拿赫定理}{哈恩－巴拿赫定理定理}、%
\href{https://zh.wikipedia.org/wiki/里斯表示定理}{里斯表示定理}和一些傅里叶分析
证明了这个结果。如果你是数学家，这个证明应该不大难理解，但对于大多数人还是很困难
的。这不得不算是一种遗憾，因为这个普遍性背后的原理其实是简单而美妙的。

在本章，我会给这个普遍性定理简单且大部分为可视化的解释。我们会一步步深入背后的思
想。你会理解为何神经网络可以计算任何的函数。你会理解到这个结论的一些局限。并且你
还会理解这些结论如何和深度神经网络关联的。

要跟随本章的内容，你不需要读过本书前面的章节。相反，本章其实可以当成独立的短文阅
读。如果你已经对神经网络有了一点基本的了解，你应该能够弄清楚这些解释。然而，我偶
尔也会给出一些联系到前面的章节的链接，帮助你填补一些知识结构的空白。

普遍性定理在计算机科学领域中常常会有，太多了以至于我们有时候都忘了这些定理的特别
之处。但值得提醒自己的是：计算任意函数的能力真是太赞了。几乎你可以想象的任何过程
都可以看做是函数的计算。考虑给一段音乐用短的音乐片段进行命名这个问题。这其实也能
够看做是计算一个函数。或者考虑将中文文本翻译成英文。同样，这又可以看成是计算一个
函数\footnote{实际上可以看成是计算很多的函数，因为对于一段文本来说有很多种翻译。}。
又或者根据一个 mp4 视频文件生成一个描述电影情节并讨论表演质量的问题。同样，这些
也都可以看成是一种类型的函数计算\footnote{Ditto the remark about translation and
  there being many possible functions.}。普遍性是指，在原理上，神经网络可以做所
有这些事情，或者更多。

当然，仅仅因为我们知道存在一个可以将中文翻译成英文的神经网络，这并不意味着我们有
了一种构造甚至识别出这样的网络的很好的技术。这个局限同样可以应用在诸如布尔电路上
的传统的普遍性定理上。但是，如同我们在本书前面看到的那样，神经网络拥有强大的算法
来学习函数。学习算法和普遍性的结合是一种有趣的混合。直到现在，本书一直是着重谈学
习算法。到了本章，我们来看看普遍性，看看它究竟意味着什么。

\section{两个预先声明}
\label{sec:two_caveats}

在解释为何普遍性定理成立前，我想要提下关于非正式的表述``神经网络可以计算任何函
数''的两个预先声明。

第一点，这句话不是说一个网络可以被用来\emph{准确地}计算任何函数。而是说，我们可
以获得尽可能好的一个\emph{近似}。通过增加隐藏元的数量，我们可以提升近似的精度。
例如，\hyperref[basic_network_precursor]{之前}我举例说明一个使用了三个隐藏元的网
络来计算 $f(x)$。对于大多数函数使用三个隐藏元仅仅能得到一个低质量的近似。通过增
加隐藏神经元的数量（比如说，五个），我们能够明显地得到更好的近似：
\begin{center}
  \includegraphics{bigger_network}
\end{center}

并且我们可以继续增加隐藏神经元的数目。

为了让这个表述更加准确，假设我们给定一个需要按照目标精度 $\epsilon > 0$ 的函数
$f(x)$。通过使用足够多的隐藏神经元使得神经网络的输出 $g(x)$ 对所有的 $x$，满足
$|g(x) - f(x)| < \epsilon$ 从而实现近似计算。换言之，近似对每个可能的输入都是限
制在目标准确度范围内的。

第二点，就是可以按照上面的方式近似的函数类其实是\emph{连续}函数。如果函数不是连
续的，也就是会有突然、极陡的跳跃，那么一般来说无法使用一个神经网络进行近似。这并
不意外，因为神经网络计算的就是输入的连续函数。然而，即使那些我们真的想要计算的函
数是不连续的，一般来说连续的近似其实也足够的好了。如果这样的话，我们就可以用神经
网络来近似了。实践中，这通常不是一个严重的限制。

总结一下，更加准确的关于普遍性定理的表述是包含一个隐藏层的神经网络可以被用来按照
任意给定的精度来近似任何连续函数。本章，我们会使用了两个隐藏层的网络来证明这个结
果的弱化的版本。在问题中我将简要介绍如何通过一些微调把这个解释适应于只使用一个隐
藏层的网络的证明。

\section{一个输入和一个输出的普遍性}
\label{sec:universality_with_one_input_and_one_output}

为了理解为何普遍性定理成立，我们先从理解如何构造这样一个神经网络开始，它能够近似
一个只有一个输入和一个输出的函数：
\begin{center}
  \includegraphics{function}
\end{center}

结果表明，这其实是普遍性问题的核心。一旦我们理解了这个特例，那么实际上就会很容易
扩展到那些有多个输入输出的函数上。

为了构建关于如何构造一个计算 $f$ 的网络的理解，让我们从一个只包含一个隐藏层的网络
开始，它有两个隐藏神经元，和单个输出神经元的输出层：
\begin{center}
  \includegraphics{two_hidden_neurons}
\end{center}

为了感受一下网络的组成部分工作的机制，我们聚焦在最顶上的那个隐藏神经元。在下图例
子中，点击权重，$w$，将鼠标从左往右拉动可以增加权重 $w$。你可以立即看到最上面的隐
藏神经元变化如何计算函数：
\begin{center}
  \includegraphics{basic_manipulation}
\end{center}

正如我们在\hyperref[sec:sigmoid_neurons]{本书前面}学到的，隐藏神经元在计算的是
$\sigma(wx + b)$，这里 $\sigma(z) \equiv 1/(1+e^{-z})$ 是 S 型函数。目前为止，我
们已经频繁使用了这个代数形式。但是为了证明普遍性，我们会完全忽略其代数性质，取而
代之的是在图形中调整和观察形状来获取更多的领悟。这不仅会给我们更好的感性认识，而
且它也会给我们一个可应用于除了S型函数之外其它激活函数的普遍性的证明\footnote{严
  格地说，我所采取的可视化方式传统上并不被认为是一个证明。但我相信可视化的方法比
  一个传统的证明能给出更深入的了解。当然，这种了解是证明背后的真正目的。偶尔，在
  我呈现的推理中会有小的差距：那些有可视化参数的地方，看上去合理，但不是很严谨。
  如果这让你烦恼，那就把它视为一个挑战，来填补这些缺失的步骤。但不要忽视了真正的
  目的：了解为什么普遍性定理是正确的。}。

开始，试着点击上面图像中的偏差 $b$，往右拖动来增加它。你能看到当偏差增加时，图形
往左移动，但是形状不变。

接下来，点击并往左拖动来减小偏差。你能看到当偏差减小时图形往右移动，但是再一次，
它的形状没有变化。

继续，将权重减小到大约 $2$ 或 $3$。你能看到当你减小权重时，曲线往两边拉宽了。你可
以同时改变偏差，让曲线保持在框内。

最后，把权重增加到超过 $w = 100$。当你这样做时，曲线变得越来越陡，直到最终看上去
就像是一个阶跃函数。试着调整偏差，使得阶跃位置靠近 $x = 0.3$。下面一个短片显示你
应该看到的结果。点击播放按钮来播放（或重播）视频：
\begin{center}
  \includegraphics{create_step_function}
\end{center}

% insert video

你能给权重增加很大的值来简化我们的分析，使得输出实际上是个非常接近的阶跃函数。下
面我画出了当权重为 $w = 999$ 时从顶部隐藏神经元的输出。注意这个图是静态的，你不能
改动如权重等参数。
\begin{center}
  \includegraphics{high_weight_function}
\end{center}

It's actually quite a bit easier to work with step functions than general
sigmoid functions. The reason is that in the output layer we add up
contributions from all the hidden neurons. It's easy to analyze the sum of a
bunch of step functions, but rather more difficult to reason about what happens
when you add up a bunch of sigmoid shaped curves. And so it makes things much
easier to assume that our hidden neurons are outputting step functions. More
concretely, we do this by fixing the weight w w to be some very large value, and
then setting the position of the step by modifying the bias. Of course, treating
the output as a step function is an approximation, but it's a very good
approximation, and for now we'll treat it as exact. I'll come back later to
discuss the impact of deviations from this approximation.

实际上处理阶跃函数比一般的S型函数更加容易。原因是在输出层我们把所有隐藏神经元的贡
献值加在一起。分析一批阶跃函数的和是容易的，相反，思考把一批S型曲线的和要更困难
些。

At what value of $x$ does the step occur? Put another way, how does the position
of the step depend upon the weight and bias?

To answer this question, try modifying the weight and bias in the diagram above
(you may need to scroll back a bit). Can you figure out how the position of the
step depends on $w$ and $b$? With a little work you should be able to convince
yourself that the position of the step is proportional to $b$, and inversely
proportional to $w$.

In fact, the step is at position $s = -b/w$, as you can see by modifying the
weight and bias in the following diagram:

% insert graph

It will greatly simplify our lives to describe hidden neurons using just a
single parameter, $s$, which is the step position, $s = -b/w$. Try modifying $s$
in the following diagram, in order to get used to the new parameterization:

% insert graph

As noted above, we've implicitly set the weight $w$ on the input to be some
large value - big enough that the step function is a very good approximation. We
can easily convert a neuron parameterized in this way back into the conventional
model, by choosing the bias $b = -w s$.

Up to now we've been focusing on the output from just the top hidden
neuron. Let's take a look at the behavior of the entire network. In particular,
we'll suppose the hidden neurons are computing step functions parameterized by
step points $s_1$ (top neuron) and $s_2$ (bottom neuron). And they'll have
respective output weights $w_1$ and $w_2$. Here's the network:

What's being plotted on the right is the weighted output $w_1 a_1 + w_2 a_2$
from the hidden layer. Here, $a_1$ and $a_2$ are the outputs from the top and
bottom hidden neurons, respectively\footnote{Note, by the way, that the output
  from the whole network is $\sigma(w_1 a_1+w_2 a_2 + b)$, where $b$ is the bias
  on the output neuron. Obviously, this isn't the same as the weighted output
  from the hidden layer, which is what we're plotting here. We're going to focus
  on the weighted output from the hidden layer right now, and only later will we
  think about how that relates to the output from the whole network.}. These
outputs are denoted with a a s because they're often known as the neurons'
activations.

Try increasing and decreasing the step point $s_1$ of the top hidden neuron. Get
a feel for how this changes the weighted output from the hidden layer. It's
particularly worth understanding what happens when $s_1$ goes past $s_2$. You'll
see that the graph changes shape when this happens, since we have moved from a
situation where the top hidden neuron is the first to be activated to a
situation where the bottom hidden neuron is the first to be activated.

Similarly, try manipulating the step point $s_2$ of the bottom hidden neuron,
and get a feel for how this changes the combined output from the hidden neurons.

Try increasing and decreasing each of the output weights. Notice how this
rescales the contribution from the respective hidden neurons. What happens when
one of the weights is zero?

Finally, try setting $w_1$ to be $0.8$ and $w_2$ to be $−0.8$. You get a
``bump'' function, which starts at point $s_1$, ends at point $s_2$, and has
height $0.8$. For instance, the weighted output might look like this:

\section{很多个输入变量}
\label{sec:many_input_variables}

\section{S 型神经元的延伸}
\label{sec:extension_beyond_sigmoid_neurons}

\section{Fixing up the step functions}
\label{sec:fixing_up_the_step_functions}

\section{结论}
\label{sec:conclusion}
