% file: chap4.tex

\chapter{目视神经网络可以计算任何函数}
\label{ch:VisualProof}

神经网络的一个最显著的事实就是它可以计算任何的函数。也就是说，假设某个人给你某种
复杂而奇特的函数，$f(x)$：
\begin{center}
  \includegraphics{function}
\end{center}

\label{basic_network_precursor}不管这个函数是什么样的，总会确保有一个神经网络能够
对任何可能的输入 $x$，其值 $f(x)$ （或者某个足够准确的近似）是网络的输出，例如：
\begin{center}
  \includegraphics{basic_network}
\end{center}

即使函数有很多输入和输出，$f = f(x_1, \ldots, x_m)$，这个结果都是成立的。例如，这
个网络计算一个具有 $m = 3$ 个输入和 $n = 2$ 个输出的函数：
\begin{center}
  \includegraphics{vector_valued_network}
\end{center}

结果表明神经网络拥有一种\emph{普遍性}。不论我们想要计算什么样的函数，我们都确信存
在一个神经网络可以计算它。

而且，这个普遍性定理甚至在我们限制了神经网络只在输入层和输出层存在一个中间层的情
况下成立。所以即使是很简单的网络架构都极其强大。

普遍性定理在使用神经网络的人群中是众所周知的。但是它为何正确却不被广泛地理解。现
有的大多数的解释都具有很强的技术性。例如，有一篇原始的论
文\footnote{Approximation by superpositions of a sigmoidal function, by George
  Cybenko (1989). The result was very much in the air at the time, and several
  groups proved closely related results. Cybenko's paper contains a useful
  discussion of much of that work. Another important early paper is Multilayer
  feedforward networks are universal approximators, by Kurt Hornik, Maxwell
  Stinchcombe, and Halbert White (1989). This paper uses the Stone-Weierstrass
  theorem to arrive at similar results.}使用
了\href{https://zh.wikipedia.org/wiki/哈恩－巴拿赫定理}{哈恩－巴拿赫定理定
  理}、\href{https://zh.wikipedia.org/wiki/里斯表示定理}{里斯表示定理}和一些傅里
叶分析证明了这个结果。如果你是数学家，这个证明应该不大难理解，但对于大多数人还是
很困难的。这不得不算是一种遗憾，因为这个普遍性背后的原理其实是简单而美妙的。

在本章，我会给这个普遍性定理简单且大部分为可视化的解释。我们会一步步深入背后的思
想。你会理解为何神经网络可以计算任何的函数。你会理解到这个结论的一些局限。并且你
还会理解这些结论如何和深度神经网络关联的。

要跟随本章的内容，你不需要读过本书前面的章节。相反，本章其实可以当成独立的短文阅
读。如果你已经对神经网络有了一点基本的了解，你应该能够弄清楚这些解释。然而，我偶
尔也会给出一些联系到前面的章节的链接，帮助你填补一些知识结构的空白。

普遍性定理在计算机科学领域中常常会有，太多了以至于我们有时候都忘了这些定理的特别
之处。但值得提醒自己的是：计算任意函数的能力真是太赞了。几乎你可以想象的任何过程
都可以看做是函数的计算。考虑给一段音乐用短的音乐片段进行命名这个问题。这其实也能
够看做是计算一个函数。或者考虑将中文文本翻译成英文。同样，这又可以看成是计算一个
函数\footnote{实际上可以看成是计算很多的函数，因为对于一段文本来说有很多种翻译。}。
又或者根据一个 mp4 视频文件生成一个描述电影情节并讨论表演质量的问题。同样，这些也
都可以看成是一种类型的函数计算\footnote{Ditto the remark about translation and
  there being many possible functions.}。普遍性是指，在原理上，神经网络可以做所有
这些事情，或者更多。

当然，仅仅因为我们知道存在一个可以将中文翻译成英文的神经网络，这并不意味着我们有
了一种构造甚至识别出这样的网络的很好的技术。这个局限同样可以应用在诸如布尔电路上
的传统的普遍性定理上。但是，如同我们在本书前面看到的那样，神经网络拥有强大的算法
来学习函数。学习算法和普遍性的结合是一种有趣的混合。直到现在，本书一直是着重谈学
习算法。到了本章，我们来看看普遍性，看看它究竟意味着什么。

\section{两个预先声明}
\label{sec:two_caveats}

在解释为何普遍性定理成立前，我想要提下关于非正式的表述``神经网络可以计算任何函
数''的两个预先声明。

第一点，这句话不是说一个网络可以被用来\emph{准确地}计算任何函数。而是说，我们可以
获得尽可能好的一个\emph{近似}。通过增加隐藏元的数量，我们可以提升近似的精度。例
如，\hyperref[basic_network_precursor]{之前}我举例说明一个使用了三个隐藏元的网络
来计算 $f(x)$。对于大多数函数使用三个隐藏元仅仅能得到一个低质量的近似。通过增加隐
藏神经元的数量（比如说，五个），我们能够明显地得到更好的近似：
\begin{center}
  \includegraphics{bigger_network}
\end{center}

并且我们可以继续增加隐藏神经元的数目。

为了让这个表述更加准确，假设我们给定一个需要按照目标精度 $\epsilon > 0$ 的函
数 $f(x)$。通过使用足够多的隐藏神经元使得神经网络的输出 $g(x)$ 对所有的 $x$，满
足 $|g(x) - f(x)| < \epsilon$ 从而实现近似计算。换言之，近似对每个可能的输入都是
限制在目标准确度范围内的。

第二点，就是可以按照上面的方式近似的函数类其实是\emph{连续}函数。如果函数不是连续
的，也就是会有突然、极陡的跳跃，那么一般来说无法使用一个神经网络进行近似。这并不
意外，因为神经网络计算的就是输入的连续函数。然而，即使那些我们真的想要计算的函数
是不连续的，一般来说连续的近似其实也足够的好了。如果这样的话，我们就可以用神经网
络来近似了。实践中，这通常不是一个严重的限制。

总结一下，更加准确的关于普遍性定理的表述是包含一个隐藏层的神经网络可以被用来按照
任意给定的精度来近似任何连续函数。本章，我们会使用了两个隐藏层的网络来证明这个结
果的弱化的版本。在问题中我将简要介绍如何通过一些微调把这个解释适应于只使用一个隐
藏层的网络的证明。

\section{一个输入和一个输出的普遍性}
\label{sec:universality_with_one_input_and_one_output}

为了理解为何普遍性定理成立，我们先从理解如何构造这样一个神经网络开始，它能够近似
一个只有一个输入和一个输出的函数：
\begin{center}
  \includegraphics{function}
\end{center}

结果表明，这其实是普遍性问题的核心。一旦我们理解了这个特例，那么实际上就会很容易
扩展到那些有多个输入输出的函数上。

为了构建关于如何构造一个计算 $f$ 的网络的理解，让我们从一个只包含一个隐藏层的网络
开始，它有两个隐藏神经元，和单个输出神经元的输出层：
\begin{center}
  \includegraphics{two_hidden_neurons}
\end{center}

为了感受一下网络的组成部分工作的机制，我们聚焦在最顶上的那个隐藏神经元。在下图例
子中，点击权重，$w$，将鼠标从左往右拉动可以增加权重 $w$。你可以立即看到最上面的隐
藏神经元变化如何计算函数：
\begin{center}
  \includegraphics{basic_manipulation}
\end{center}

正如我们在\hyperref[sec:sigmoid_neurons]{本书前面}学到的，隐藏神经元在计算的
是 $\sigma(wx + b)$，这里 $\sigma(z) \equiv 1/(1+e^{-z})$ 是 S 型函数。Up to
now, we've made frequent use of this algebraic form. But for the proof of
universality we will obtain more insight by ignoring the algebra entirely, and
instead manipulating and observing the shape shown in the graph. This won't just
give us a better feel for what's going on, it will also give us a
proof\footnote{ Strictly speaking, the visual approach I'm taking isn't what's
  traditionally thought of as a proof. But I believe the visual approach gives
  more insight into why the result is true than a traditional proof. And, of
  course, that kind of insight is the real purpose behind a proof. Occasionally,
  there will be small gaps in the reasoning I present: places where I make a
  visual argument that is plausible, but not quite rigorous. If this bothers
  you, then consider it a challenge to fill in the missing steps. But don't lose
  sight of the real purpose: to understand why the universality theorem is
  true.} of universality that applies to activation functions other than the
sigmoid function.

To get started on this proof, try clicking on the bias, $b$, in the diagram
above, and dragging to the right to increase it. You'll see that as the bias
increases the graph moves to the left, but its shape doesn't change.

Next, click and drag to the left in order to decrease the bias. You'll see that
as the bias decreases the graph moves to the right, but, again, its shape
doesn't change.

Next, decrease the weight to around $2$ or $3$. You'll see that as you decrease
the weight, the curve broadens out. You might need to change the bias as well,
in order to keep the curve in-frame.

Finally, increase the weight up past $w = 100$. As you do, the curve gets
steeper, until eventually it begins to look like a step function. Try to adjust
the bias so the step occurs near $x = 0.3$. The following short clip shows what
your result should look like. Click on the play button to play (or replay) the
video:

% insert video

We can simplify our analysis quite a bit by increasing the weight so much that
the output really is a step function, to a very good approximation. Below I've
plotted the output from the top hidden neuron when the weight is $w = 999$. Note
that this plot is static, and you can't change parameters such as the weight.
\begin{center}
  \includegraphics{high_weight_function}
\end{center}

% insert graph

It's actually quite a bit easier to work with step functions than general
sigmoid functions. The reason is that in the output layer we add up
contributions from all the hidden neurons. It's easy to analyze the sum of a
bunch of step functions, but rather more difficult to reason about what happens
when you add up a bunch of sigmoid shaped curves. And so it makes things much
easier to assume that our hidden neurons are outputting step functions. More
concretely, we do this by fixing the weight w w to be some very large value, and
then setting the position of the step by modifying the bias. Of course, treating
the output as a step function is an approximation, but it's a very good
approximation, and for now we'll treat it as exact. I'll come back later to
discuss the impact of deviations from this approximation.

At what value of $x$ does the step occur? Put another way, how does the position
of the step depend upon the weight and bias?

To answer this question, try modifying the weight and bias in the diagram above
(you may need to scroll back a bit). Can you figure out how the position of the
step depends on $w$ and $b$? With a little work you should be able to convince
yourself that the position of the step is proportional to $b$, and inversely
proportional to $w$.

In fact, the step is at position $s = -b/w$, as you can see by modifying the
weight and bias in the following diagram:

% insert graph

It will greatly simplify our lives to describe hidden neurons using just a
single parameter, $s$, which is the step position, $s = -b/w$. Try modifying $s$
in the following diagram, in order to get used to the new parameterization:

% insert graph

\section{很多个输入变量}
\label{sec:many_input_variables}

\section{S 型神经元的延伸}
\label{sec:extension_beyond_sigmoid_neurons}

\section{Fixing up the step functions}
\label{sec:fixing_up_the_step_functions}

\section{结论}
\label{sec:conclusion}
