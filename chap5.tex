% file: chap5.tex

\chapter{深度神经网络为何很难训练}
\label{ch:WhyHardToTrain}

假设你是一名工程师，接到一项从头开始设计计算机的任务。某天，你在工作室工作，设计
逻辑电路，构建{\bfseries\SourceSerifPro AND}门，{\bfseries\SourceSerifPro OR}门等
等时，老板带着坏消息进来：客户刚刚添加了一个奇特的设计需求：整个计算机的线路的深
度必须只有两层：
\begin{center}
  \includegraphics{shallow_circuit}
\end{center}

你惊呆了，跟老板说道：“这货疯掉了吧！”
 
老板说：“我也认为他们疯了，但是客户的需求比天大，我们要满足它。”
 
实际上，在某种程度上看，他们的客户并没有太疯狂。假设你可以使用某种特殊的逻辑门，
它让你对任意多的输入做{\bfseries\SourceSerifPro AND}运算。同样也能使用多输入
的{\bfseries\SourceSerifPro NAND}门 —— 可以对多个输入做{\bfseries\SourceSerifPro
  AND}运算并取负的门。有了这类特殊的门，构建出来的两层深度的电路可以计算任何函
数。

但是仅仅因为某件事是理论上可能的，并不代表这是一个好的想法。在实践中，在解决线路
设计问题（或者大多数的其他算法问题）时，我们通常考虑如何解决子问题，然后逐步地集
成这些子问题的解。换句话说，我们通过多层的抽象来获得最终的解答。

例如，假设我们来设计一个逻辑线路来做两个数的乘法。我们希望在已经有了计算两个数加
法的子线路基础上创建这个逻辑线路。计算两个数和的子线路也是构建在用于两个比特相加
的子子线路上的。粗略地讲我们的线路看起来像这个样子：
\begin{center}
  \includegraphics{circuit_multiplication}
\end{center}

\begin{center}
  \includegraphics{tikz35}
\end{center}

\begin{center}
  \includegraphics{tikz36}
\end{center}

\begin{center}
  \includegraphics{tikz37}
\end{center}

\begin{center}
  \includegraphics{tikz38}
\end{center}

\begin{center}
  \includegraphics{tikz39}
\end{center}

\section{消失的梯度问题}
\label{sec:the_vanishing_gradient_problem}

\section{什么导致了消失的梯度问题？也就是在深度神经网络中的所谓的梯度不稳定性}

\section{在更加复杂网络中的不稳定梯度}

\section{其它深度学习的障碍}
